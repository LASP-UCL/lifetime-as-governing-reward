import gym
import minihack
from nle import nethack
import tqdm
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import time
import pygame
import IPython.display
import os

import sys
sys.path.append('../')
from models import DiscreteActorNet

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_PATH = 'actor.pt'
HIDDEN_SIZE = 32
MAX_TIMESTEPS = 1000

# Minihack hyperparams
ROOM_TYPE = "" #"", "Random", "Dark", "Monster", "Trap, "Ultimate"
ROOM_SIZE = "5x5" #"5x5", "15x15"
room_str = f'{ROOM_TYPE+"-" if ROOM_TYPE!="" else ""}{ROOM_SIZE}'
ENV_NAME = f'MiniHack-Room-{room_str}-v0'
ACTION_KEYS = tuple(nethack.CompassDirection)  # Restrict actions to movement only
OBS_KEYS = ("glyphs_crop", "blstats")
# Define room minihack environment
env = gym.make(ENV_NAME,
               actions=ACTION_KEYS,
               observation_keys=OBS_KEYS
)

print("Observation space:", env.observation_space)
print("Action space:", env.action_space)

# Load model searching in trained-models folder (filename containing actor_{room_str})
try: 
    actor_filename = [f for f in os.listdir('trained-models') if f'actor_{room_str}' in f][0]
    print("Loading model from", actor_filename)
    actor_net = torch.load("trained-models/"+actor_filename)
except:
    raise Exception("No saved model found")

# Initialise environment state
state = env.reset()
state_tensor = torch.cat([torch.tensor(state[key].flatten(), dtype=torch.float32) for key in state.keys()])
total_reward, ep = 0, 0
render_strings = []

compass = {0: 'N', 1: 'E', 2: 'S', 3: 'W', 4: 'NE', 5: 'SE', 6: 'SW', 7: 'NW'}

def get_render_string():
    env_render = env.render(mode='ansi')
    env_render = env_render.replace(' ', '').replace('>', 'G').replace('<', 'S')
    first_char = min(env_render.find("."), env_render.find("S"), env_render.find("G"))
    last_char = max(env_render.rfind("."), env_render.rfind("S"), env_render.rfind("G"))
    return env_render[first_char:last_char+1]
    
for step in range(MAX_TIMESTEPS):
    render_strings.append(get_render_string())
    action = actor_net.get_action(state_tensor)[0]
    state, reward, done, _ = env.step(action)
    render_strings.append(f"Move {compass[action.item()]}, reward {reward}")
    state_tensor = torch.cat([torch.tensor(state[key].flatten(), dtype=torch.float32) for key in state.keys()])
    total_reward += reward

    if done:
        print(f"Episode {ep} finished | Reward {total_reward}\n")
        #break
        state = env.reset()
        state_tensor = torch.cat([torch.tensor(state[key].flatten(), dtype=torch.float32) for key in state.keys()])
        ep += 1
        total_reward = 0
        time.sleep(1)
env.close()

# Write render strings to file
with open('render_strings.txt', 'w') as f:
    for render_string in render_strings:
        f.write(render_string + '\n\n')