import gym
import minihack
from nle import nethack
import tqdm
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import time
import pygame

import sys
sys.path.append('../')
from models import DiscreteActorNet

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_PATH = 'actor.pt'
HIDDEN_SIZE = 32
MAX_TIMESTEPS = 1000

# Restrict actions to movement and navigation
MOVE_ACTIONS = tuple(nethack.CompassDirection)

# Define room minihack environment
env = gym.make('MiniHack-Room-5x5-v0',
               actions=MOVE_ACTIONS,
               observation_keys=("glyphs", "chars", "colors")
)

print("Observation space:", env.observation_space)
print("Action space:", env.action_space)

# Define models
#actor_net = torch.load('actor.pth')

# Initialise environment state
#state = env.reset(seed=42)
state = env.reset()
total_reward, ep = 0, 0

for step in range(MAX_TIMESTEPS):
    # Select action at random
    action = env.action_space.sample()
    state, reward, done, _ = env.step(action)
    state_tensor = torch.cat([torch.tensor(state[key].flatten()) for key in state.keys()])
    total_reward += reward
    env.render()
    time.sleep(0.1)
    if done:
        state = env.reset()
        state_tensor = torch.cat([torch.tensor(state[key].flatten()) for key in state.keys()])
        ep += 1
        print(f"Episode {ep} finished | Reward {total_reward}\n")
        total_reward = 0
        time.sleep(1)
env.close()